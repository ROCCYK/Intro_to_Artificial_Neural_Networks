{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training â€ŠWord2Vec Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training the embedding layer, we can first separately learn word embeddings and then pass to the embedding layer. This approach also allows to use any pre-trained word embedding and also saves the time in training the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a recreational golfer with some knowledge o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I went and saw this movie last night after bei...          1\n",
       "1  Actor turned director Bill Paxton follows up h...          1\n",
       "2  As a recreational golfer with some knowledge o...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Split\n",
      "1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Class Balance\n",
    "print('Class Split')\n",
    "print(df['sentiment'].value_counts())\n",
    "df['sentiment'].value_counts().plot.bar(figsize=(6,4),title='Classes Split for Dataset')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['review'].values.tolist()\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(line)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensimâ€™s Word2Vec API requires some parameters for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 134095\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "EMBEDDING_DIM = 100\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model in ASCII (word2vec) format\n",
    "filename = 'imdb_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.9237805008888245),\n",
       " ('awful', 0.8649317026138306),\n",
       " ('horrendous', 0.7769838571548462),\n",
       " ('sucks', 0.7600740194320679),\n",
       " ('pathetic', 0.7443255186080933),\n",
       " ('lousy', 0.7442372441291809),\n",
       " ('dreadful', 0.7406150102615356),\n",
       " ('atrocious', 0.7363724112510681),\n",
       " ('horrid', 0.7342178821563721),\n",
       " ('bad', 0.7320282459259033)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us try some utility functions of gensim word2vec \n",
    "model.wv.most_similar('horrible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romeo', 0.9039229154586792),\n",
       " ('juliet', 0.8962530493736267),\n",
       " ('princess', 0.871935248374939),\n",
       " ('jerol', 0.8597689270973206),\n",
       " ('godmother', 0.8468965888023376),\n",
       " ('queen', 0.8447667360305786),\n",
       " ('mtvfashioned', 0.8424580097198486),\n",
       " ('shepis', 0.8379086852073669),\n",
       " ('carmen', 0.8372462391853333),\n",
       " ('manwife', 0.8339641690254211)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Letâ€™s see the result of semantically reasonable word vectors (king - man + woman)\n",
    "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n"
     ]
    }
   ],
   "source": [
    "#odd word out\n",
    "print(model.wv.doesnt_match(\"woman king queen movie\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "#odd word out\n",
    "print(model.wv.doesnt_match(\"man king women people \".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mouse', 0.8166879415512085),\n",
       " ('dog', 0.7927877306938171),\n",
       " ('dude', 0.7037979364395142),\n",
       " ('monkey', 0.7003536820411682),\n",
       " ('hat', 0.695722222328186),\n",
       " ('pet', 0.6832510828971863),\n",
       " ('rabbit', 0.6791969537734985),\n",
       " ('snake', 0.6791744828224182),\n",
       " ('fraidy', 0.6790204048156738),\n",
       " ('bugs', 0.6738077402114868)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8268802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noopa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('boy', 'girl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pre-trained Embedding to train the model.Since we have already trained word2vec model with IMDb dataset, we have the word embeddings ready to use. The next step is to load the word embedding as a directory of words to vectors. The word embedding was saved in file imdb_embedding_word2vec.txt. Let us extract the word embeddings from the stored file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'imdb_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the word embedding into tokenized vector. Recall that the review documents are integer encoded prior to passing them to the Embedding layer. The integer maps to the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map to the correct vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134095 unique tokens.\n",
      "Shape of review tensor: (50000, 100)\n",
      "Shape of sentiment tensor: (50000,)\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df['sentiment'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map embeddings from the loaded word2vec model for each word to the *tokenizer_obj.word_index* vocabulary and create a matrix with of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134096\n"
     ]
    }
   ],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the built model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          13409600  \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,422,401\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 13,409,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that model total params = 13,422,401 but trainable params = 12801. Since the model uses pre-trained word embedding it has very few trainable params and hence should train faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad tensor: (40000, 100)\n",
      "Shape of y_train tensor: (40000,)\n",
      "Shape of X_test_pad tensor: (10000, 100)\n",
      "Shape of y_test tensor: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/25\n",
      " - 13s - loss: 0.6620 - accuracy: 0.6352 - val_loss: 0.6051 - val_accuracy: 0.6695\n",
      "Epoch 2/25\n",
      " - 12s - loss: 0.5859 - accuracy: 0.6956 - val_loss: 0.5780 - val_accuracy: 0.6976\n",
      "Epoch 3/25\n",
      " - 13s - loss: 0.5834 - accuracy: 0.7084 - val_loss: 0.5967 - val_accuracy: 0.6813\n",
      "Epoch 4/25\n",
      " - 13s - loss: 0.5730 - accuracy: 0.7174 - val_loss: 0.5923 - val_accuracy: 0.6837\n",
      "Epoch 5/25\n",
      " - 12s - loss: 0.5463 - accuracy: 0.7279 - val_loss: 0.5774 - val_accuracy: 0.7011\n",
      "Epoch 6/25\n",
      " - 12s - loss: 0.5314 - accuracy: 0.7385 - val_loss: 0.5677 - val_accuracy: 0.7077\n",
      "Epoch 7/25\n",
      " - 13s - loss: 0.5171 - accuracy: 0.7551 - val_loss: 0.5655 - val_accuracy: 0.7143\n",
      "Epoch 8/25\n",
      " - 13s - loss: 0.6299 - accuracy: 0.7442 - val_loss: 0.5803 - val_accuracy: 0.6993\n",
      "Epoch 9/25\n",
      " - 12s - loss: 0.5614 - accuracy: 0.7369 - val_loss: 0.5705 - val_accuracy: 0.7064\n",
      "Epoch 10/25\n",
      " - 12s - loss: 0.5332 - accuracy: 0.7515 - val_loss: 0.5672 - val_accuracy: 0.7130\n",
      "Epoch 11/25\n",
      " - 12s - loss: 0.5090 - accuracy: 0.7644 - val_loss: 0.5664 - val_accuracy: 0.7162\n",
      "Epoch 12/25\n",
      " - 13s - loss: 0.5056 - accuracy: 0.7649 - val_loss: 0.5615 - val_accuracy: 0.7197\n",
      "Epoch 13/25\n",
      " - 13s - loss: 0.4991 - accuracy: 0.7701 - val_loss: 0.5606 - val_accuracy: 0.7201\n",
      "Epoch 14/25\n",
      " - 13s - loss: 0.4908 - accuracy: 0.7731 - val_loss: 0.5601 - val_accuracy: 0.7212\n",
      "Epoch 15/25\n",
      " - 13s - loss: 0.5299 - accuracy: 0.7580 - val_loss: 0.5862 - val_accuracy: 0.6911\n",
      "Epoch 16/25\n",
      " - 13s - loss: 0.5169 - accuracy: 0.7551 - val_loss: 0.5650 - val_accuracy: 0.7160\n",
      "Epoch 17/25\n",
      " - 13s - loss: 0.6212 - accuracy: 0.7451 - val_loss: 0.5896 - val_accuracy: 0.6838\n",
      "Epoch 18/25\n",
      " - 13s - loss: 0.5333 - accuracy: 0.7498 - val_loss: 0.5788 - val_accuracy: 0.6998\n",
      "Epoch 19/25\n",
      " - 12s - loss: 0.5388 - accuracy: 0.7511 - val_loss: 0.5747 - val_accuracy: 0.7038\n",
      "Epoch 20/25\n",
      " - 13s - loss: 0.5094 - accuracy: 0.7663 - val_loss: 0.5694 - val_accuracy: 0.7078\n",
      "Epoch 21/25\n",
      " - 13s - loss: 0.7429 - accuracy: 0.7448 - val_loss: 0.6336 - val_accuracy: 0.6356\n",
      "Epoch 22/25\n",
      " - 13s - loss: 0.5945 - accuracy: 0.7154 - val_loss: 0.5869 - val_accuracy: 0.6929\n",
      "Epoch 23/25\n",
      " - 13s - loss: 0.5636 - accuracy: 0.7215 - val_loss: 0.5923 - val_accuracy: 0.6800\n",
      "Epoch 24/25\n",
      " - 13s - loss: 0.5701 - accuracy: 0.7475 - val_loss: 0.5731 - val_accuracy: 0.7057\n",
      "Epoch 25/25\n",
      " - 13s - loss: 0.5387 - accuracy: 0.7615 - val_loss: 0.5680 - val_accuracy: 0.7159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2528e407448>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "10000/10000 [==============================] - 1s 112us/step\n",
      "Validation Loss: 0.5680101370811462\n",
      "Test accuracy: 0.7159000039100647\n",
      "Accuracy: 71.59%\n"
     ]
    }
   ],
   "source": [
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Validation Loss:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6387844 ],\n",
       "       [0.55495757],\n",
       "       [0.5035512 ],\n",
       "       [0.3968063 ],\n",
       "       [0.49933943],\n",
       "       [0.24318704],\n",
       "       [0.55495757],\n",
       "       [0.32361543]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us test some  samples\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "\n",
    "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "test_sample_2 = \"Good movie!\"\n",
    "test_sample_3 = \"Maybe I like this movie.\"\n",
    "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
    "test_sample_5 = \"if you like action, then this movie might be good for you.\"\n",
    "test_sample_6 = \"Bad movie!\"\n",
    "test_sample_7 = \"Not a good movie!\"\n",
    "test_sample_8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n",
    "\n",
    "test_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n",
    "test_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=100)\n",
    "\n",
    "#predict\n",
    "model.predict(x=test_samples_tokens_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28751183] 0  Right prdiction\n",
      "[0.6935145] 1  Right prdiction\n",
      "[0.8876598] 1  Right prdiction\n",
      "[0.6565224] 1  Right prdiction\n",
      "[0.55315673] 0  Wrong prdiction\n",
      "[0.5464764] 0  Wrong prdiction\n",
      "[0.16490605] 0  Right prdiction\n",
      "[0.6375148] 0  Wrong prdiction\n",
      "[0.54393625] 1  Right prdiction\n",
      "[0.63414353] 0  Wrong prdiction\n"
     ]
    }
   ],
   "source": [
    "#let us check how the model predicts\n",
    "classes = model.predict(X_test_pad[:10], batch_size=128)\n",
    "for i in range (0,10):\n",
    "    if(classes[i] > 0.5 and y_test[i] == 1 or (classes[i] <= 0.5 and y_test[i] == 0)):\n",
    "        print( classes[i], y_test[i], \" Right prdiction\")\n",
    "    else :\n",
    "        print( classes[i], y_test[i], \" Wrong prdiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
